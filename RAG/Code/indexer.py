import sys
import os
import subprocess

def check_and_install_dependencies():
    """
    Checks for required packages and tries to install them if missing.
    """
    try:
        from langchain_community.document_loaders import PyPDFLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_chroma import Chroma
        import pypdf
        import sentence_transformers
        print("‚úÖ All dependencies are installed.")
        return
    except ImportError:
        print("‚ö†Ô∏è Missing one or more required packages. Attempting installation...")

    # Find requirements.txt
    script_dir = os.path.dirname(os.path.abspath(__file__))
    requirements_path = os.path.join(script_dir, '..', 'requirements.txt')
    if not os.path.exists(requirements_path):
        # Fallback for different CWD, e.g. running from project root
        requirements_path = os.path.join('RAG', 'requirements.txt')

    if not os.path.exists(requirements_path):
        print("‚ùå CRITICAL: Could not find 'requirements.txt'.")
        print("   Please make sure the file is present at 'NewCode/RAG/requirements.txt'.")
        sys.exit(1)

    print(f"   Installing packages from: {requirements_path}")
    try:
        # Use the same python executable that is running this script
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", requirements_path])
        print("\n‚úÖ Dependencies installed successfully.")
        print("   Please run the script again to start the indexing process.")
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå Automatic installation failed: {e}")
        print("   Please install the dependencies manually by running this command in your terminal:")
        print(f'      "{sys.executable}" -m pip install -r "{requirements_path}"')

    sys.exit(1) # Exit after attempting installation

check_and_install_dependencies()

# --- Main script starts here ---
import os
import glob
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import shutil

# --- CONFIGURATION ---
DATA_PATH = "./RAG/data_collection"   # Ton dossier avec les 4 sous-dossiers
DB_PATH = "./chroma_db"           # L√† o√π la base de donn√©es sera cr√©√©e
CLASSES = ["Maladies", "Irrigation", "Pesticides", "Recolte"]

# Mod√®le d'embedding (traducteur texte -> vecteur)
# On utilise un mod√®le l√©ger et performant en multilingue (Arabe inclus)
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

def process_documents():
    # 1. Initialiser le mod√®le d'embedding
    print(f"üîå Chargement du mod√®le d'embedding '{EMBEDDING_MODEL_NAME}'...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    # Si la DB existe d√©j√†, on la supprime pour repartir √† z√©ro (mode clean)
    if os.path.exists(DB_PATH):
        print("üóëÔ∏è  Suppression de l'ancienne base de donn√©es pour reconstruction...")
        shutil.rmtree(DB_PATH)

    # 2. Boucle sur chaque classe
    for category in CLASSES:
        print(f"\nüöÄ Traitement de la classe : {category.upper()}")
        
        # Chemin vers les fichiers de cette classe
        folder_path = os.path.join(DATA_PATH, category)
        pdf_files = glob.glob(f"{folder_path}/*.pdf")
        
        if not pdf_files:
            print(f"‚ö†Ô∏è  Aucun fichier trouv√© dans {category}")
            continue

        documents = []
        
        # A. Chargement
        for file_path in pdf_files:
            try:
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                # On ajoute des m√©tadonn√©es pour tracer la source
                for doc in docs:
                    doc.metadata["source_class"] = category
                    doc.metadata["filename"] = os.path.basename(file_path)
                documents.extend(docs)
                print(f"   üìÑ Charg√© : {os.path.basename(file_path)}")
            except Exception as e:
                print(f"   ‚ùå Erreur lecture {os.path.basename(file_path)}: {e}")

        # B. D√©coupage (Chunking)
        # 1000 caract√®res par morceau, avec 200 de chevauchement pour le contexte
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"   ‚úÇÔ∏è  D√©coup√© en {len(chunks)} morceaux (chunks).")

        # C. Stockage dans ChromaDB (Une collection par classe !)
        if chunks:
            print(f"   üíæ Indexation dans ChromaDB (Collection: {category})...")
            vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=embeddings,
                persist_directory=DB_PATH,
                collection_name=category  # <--- CRUCIAL : On cr√©e une collection s√©par√©e
            )
            print(f"   ‚úÖ Termin√© pour {category}.")

    print(f"\nüèÜ Indexation termin√©e ! La base de donn√©es est sauvegard√©e dans '{DB_PATH}'")

if __name__ == "__main__":
    process_documents()